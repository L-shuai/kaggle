{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 带有灾难推文的自然语言处理\n",
    ">预测哪些推文是关于真实灾难的，哪些不是\n",
    ">来源：https://www.kaggle.com/search?competitionId=17777\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset\\sample_submission.csv\n",
      "./dataset\\test.csv\n",
      "./dataset\\train.csv\n"
     ]
    }
   ],
   "source": [
    "# 导包\n",
    "import numpy as np\n",
    "import pandas as pd #data processing, CSV file I/O\n",
    "import os\n",
    "\n",
    "from transformers import BertTokenizer,BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm #查看进度\n",
    "from argparse import ArgumentParser\n",
    "# Ignite帮助您在几行代码中编写紧凑但功能齐全的train循环\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.engine.engine import Engine, State, Events\n",
    "from ignite.handlers import EarlyStopping\n",
    "from ignite.contrib.handlers import TensorboardLogger, ProgressBar\n",
    "from ignite.utils import convert_tensor\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for dirname,_,filenames in os.walk('./dataset'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname,filename))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 读取数据&加载Bert分词器"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "def readfiles():\n",
    "    path = 'dataset'\n",
    "    train = pd.read_csv(os.path.join(path,'train.csv'))\n",
    "    test = pd.read_csv(os.path.join(path,'test.csv'))\n",
    "    sample_subs = pd.read_csv(os.path.join(path,'sample_submission.csv'))\n",
    "\n",
    "    return train,test,sample_subs\n",
    "\n",
    "train,test,sample_subs = readfiles()\n",
    "# print(sample_subs)\n",
    "\n",
    "\n",
    "#加载Bert分词器\n",
    "from transformers import BertTokenizer\n",
    "def Bert_Tokenizer(model_name):\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    return tokenizer\n",
    "tokenizer = Bert_Tokenizer('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 制作dataset数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "#自定义dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer,max_len):\n",
    "        self.bert_encode = tokenizer\n",
    "        self.texts = df.text.values\n",
    "        self.labels = df.target.values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens,mask,tokens_len = self.get_token_mask(self.texts[idx],self.max_len)\n",
    "        label = self.labels[idx]\n",
    "        return [torch.tensor(tokens),torch.tensor(mask),torch.tensor(tokens_len)],label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def get_token_mask(self,text,max_len):\n",
    "        tokens = [] #分词\n",
    "        mask = [] #和网络中的ip掩码功能一样\n",
    "        text = self.bert_encode.encode(text)\n",
    "        size = len(text)\n",
    "        pads = self.bert_encode.encode(['PAD']*(max(0,max_len-size))) #不足的补PAD\n",
    "        tokens[:max(max_len,size)] = text[:max(max_len,size)] #超出的部分截断\n",
    "        tokens = tokens + pads[1:-1]\n",
    "        mask = [1]*size+[0]*len(pads[1:-1])\n",
    "        tokens_len = len(tokens)\n",
    "\n",
    "        return tokens,mask,tokens_len\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#加载数据集\n",
    "def get_data_loaders():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x_train , x_valid = train_test_split(train, test_size=0.1,random_state=2020)\n",
    "    train_dataset = TextDataset(x_train,tokenizer=tokenizer,max_len=120)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=1,shuffle=True)\n",
    "    valid_dataset = TextDataset(x_valid,tokenizer=tokenizer,max_len=120)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=1,shuffle=True)\n",
    "\n",
    "    return train_loader , valid_loader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 搭建Bert模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "class MixedBertModel(nn.Module):\n",
    "    def __init__(self,pre_trained='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(pre_trained)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.LSTM = nn.LSTM(self.hidden_size,self.hidden_size,bidirectional=True)\n",
    "        self.clf = nn.Linear(self.hidden_size*2,1)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "\n",
    "        encoded_layers, pooled_output = self.bert(input_ids=inputs[0],attention_mask=inputs[1])\n",
    "        encoded_layers = encoded_layers.permute(1, 0, 2)\n",
    "        enc_hiddens, (last_hidden, last_cell) = self.LSTM(pack_padded_sequence(encoded_layers, inputs[2]))\n",
    "        output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)\n",
    "        output_hidden = F.dropout(output_hidden,0.2)\n",
    "        output = self.clf(output_hidden)\n",
    "\n",
    "        return F.sigmoid(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "class MixedBertModel(nn.Module):\n",
    "    def __init__(self,pre_trained='bert-base-uncased'):\n",
    "        # 待验证\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(pre_trained)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        # 双向LSTM\n",
    "        \"\"\"\n",
    "        input_size – 输入数据的大小，也就是前面例子中每个单词向量的长度\n",
    "        hidden_size – 隐藏层的大小（即隐藏层节点数量），输出向量的维度等于隐藏节点数\n",
    "        num_layers – recurrent layer的数量，默认等于1。\n",
    "        \"\"\"\n",
    "        self.LSTM = nn.LSTM(self.hidden_size,self.hidden_size,bidirectional=True)\n",
    "        # 分类器\n",
    "        self.clf = nn.Linear(in_features=self.hidden_size*2,out_features=1)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        print(\"input.shape:\",len(inputs))\n",
    "        print(\"input:\",inputs)\n",
    "        encoded_layers, pooled_output = self.bert(input_ids=inputs[0],attention_mask=inputs[1])\n",
    "        # print(\"encoded_layers.type:\",type(encoded_layers))\n",
    "        # print(\"type:\",type(self.bert(input_ids=inputs[0],attention_mask=inputs[1])))\n",
    "        # print(\"encoded_layers:--\",encoded_layers)\n",
    "        # print(self.bert(input_ids=inputs[0],attention_mask=inputs[1]))\n",
    "        print(\"--\",len(encoded_layers))\n",
    "        print(\"++++:\",encoded_layers.squeeze(0),\"++++\")\n",
    "        # encoded_layers = torch.from_numpy(encoded_layers)\n",
    "        encoded_layers = encoded_layers.permute(1, 0, 2) #将tensor的维度换位\n",
    "        \"\"\"\n",
    "        # pack_padded_sequence:压缩填充张量  去除末尾填充的PAD，防止PAD进入模型，浪费资源\n",
    "        LSTM的输出:enc_hiddens,(last_hidden,last_cell)\n",
    "        last_hidden为最后1个time step的隐状态结果\n",
    "        last_cell为最后1个time step的cell状态结果\n",
    "        \"\"\"\n",
    "        # enc_hiddens, (last_hidden, last_cell) = self.LSTM(pack_padded_sequence(encoded_layers, inputs[2]))\n",
    "        enc_hiddens, (last_hidden, last_cell) = self.LSTM(pack_padded_sequence(encoded_layers, inputs[2]))\n",
    "        output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)\n",
    "        # 避免过拟合，并增强模型的泛化能力\n",
    "        output_hidden = F.dropout(output_hidden,0.2)\n",
    "        output = self.clf(output_hidden)\n",
    "\n",
    "        return F.sigmoid(output)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 预处理"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "#将数据集转为tensor类型\n",
    "def _prepare_batch(batch, device=None, non_blocking=False):\n",
    "\n",
    "    x, y = batch\n",
    "    return (convert_tensor(x, device=device, non_blocking=non_blocking),\n",
    "            convert_tensor(y, device=device, non_blocking=non_blocking))\n",
    "\n",
    "# 创建监督学习\n",
    "def create_supervised_trainer1(model, optimizer, loss_fn, metrics={}, device=None):\n",
    "\n",
    "    def _update(engine, batch):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()#每一轮开始前  梯度都要清零\n",
    "        x,y = _prepare_batch(batch,device=device)\n",
    "        y_pred = model(x)#模型输出值\n",
    "        loss = loss_fn(y_pred,y.float()) #计算损失值\n",
    "        loss.backward()\n",
    "        optimizer.step()#更新参数值\n",
    "        return loss.item(),y_pred,y\n",
    "\n",
    "    def _metrics_transform(output):\n",
    "        return output[1], output[2]\n",
    "\n",
    "    engine = Engine(_update)\n",
    "\n",
    "    for name, metric in metrics.items():\n",
    "        metric._output_transform = _metrics_transform\n",
    "        metric.attach(engine, name)\n",
    "\n",
    "    return engine\n",
    "\n",
    "def create_supervised_evaluator1(model, metrics=None,\n",
    "                                device=None, non_blocking=False,\n",
    "                                prepare_batch=_prepare_batch,\n",
    "                                output_transform=lambda x, y, y_pred: (y_pred, y,)):\n",
    "\n",
    "    metrics = metrics or {}\n",
    "\n",
    "    if device:\n",
    "        model\n",
    "\n",
    "    def _inference(engine, batch):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x, y = prepare_batch(batch, device=device, non_blocking=non_blocking)\n",
    "            y_pred = model(x)\n",
    "            return output_transform(x, y.float(), y_pred)\n",
    "\n",
    "    engine = Engine(_inference)\n",
    "\n",
    "    for name, metric in metrics.items():\n",
    "        metric.attach(engine, name)\n",
    "\n",
    "    return engine"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "训练模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                   \u001B[ASome weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "loss: 0.0000 | lr: 0.0000:   0%|          | 0/6851 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.shape: 3\n",
      "input: [tensor([[  101, 11865, 24917,  1005, 15527, 15415,  1005,  4871, 11740,  2408,\n",
      "          1996,  2555,  2048,  2086,  2044,  4517,  7071,  8299,  1024,  1013,\n",
      "          1013,  1056,  1012,  2522,  1013, 17816, 19481,  4887,  2509, 15042,\n",
      "          3081,  1030,  5653,  2239,  4179,   102,   100,   100,   100,   100,\n",
      "           100,   100,   100,   100,   100,   100,   100,   100,   100,   100,\n",
      "           100,   100,   100,   100,   100,   100,   100,   100,   100,   100,\n",
      "           100,   100,   100,   100,   100,   100,   100,   100,   100,   100,\n",
      "           100,   100,   100,   100,   100,   100,   100,   100,   100,   100,\n",
      "           100,   100,   100,   100,   100,   100,   100,   100,   100,   100,\n",
      "           100,   100,   100,   100,   100,   100,   100,   100,   100,   100,\n",
      "           100,   100,   100,   100,   100,   100,   100,   100,   100,   100,\n",
      "           100,   100,   100,   100,   100,   100,   100,   100,   100,   100]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), tensor([120])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current run is terminating due to exception: 'str' object has no attribute 'detach'\n",
      "Engine run is terminating due to exception: 'str' object has no attribute 'detach'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- 17\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-107-e504e3e4c461>\", line 59, in run\n",
      "    trainer.run(train_loader, max_epochs=epochs)\n",
      "  File \"E:\\IT\\code\\Python\\environment\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\ignite\\engine\\engine.py\", line 698, in run\n",
      "    return self._internal_run()\n",
      "  File \"E:\\IT\\code\\Python\\environment\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\ignite\\engine\\engine.py\", line 771, in _internal_run\n",
      "    self._handle_exception(e)\n",
      "  File \"E:\\IT\\code\\Python\\environment\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\ignite\\engine\\engine.py\", line 466, in _handle_exception\n",
      "    raise e\n",
      "  File \"E:\\IT\\code\\Python\\environment\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\ignite\\engine\\engine.py\", line 741, in _internal_run\n",
      "    time_taken = self._run_once_on_dataset()\n",
      "  File \"E:\\IT\\code\\Python\\environment\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\ignite\\engine\\engine.py\", line 845, in _run_once_on_dataset\n",
      "    self._handle_exception(e)\n",
      "  File \"E:\\IT\\code\\Python\\environment\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\ignite\\engine\\engine.py\", line 466, in _handle_exception\n",
      "    raise e\n",
      "  File \"E:\\IT\\code\\Python\\environment\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\ignite\\engine\\engine.py\", line 831, in _run_once_on_dataset\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"<ipython-input-96-642ef3c5c084>\", line 15, in _update\n",
      "    y_pred = model(x)#模型输出值\n",
      "  File \"E:\\IT\\code\\Python\\environment\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"<ipython-input-106-db5f039ee143>\", line 26, in forward\n",
      "    print(\"++++:\",encoded_layers.detach().squeeze(0),\"++++\")\n",
      "AttributeError: 'str' object has no attribute 'detach'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run(log_interval=100,epochs=2,lr=0.000006):\n",
    "    train_loader ,valid_loader = get_data_loaders()\n",
    "    model = MixedBertModel()\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    criterion = nn.BCELoss() #损失函数\n",
    "    optimizer = AdamW(model.parameters(),lr=lr) #优化器\n",
    "    # 学习率衰减，每个epoch中lr都乘以gamma\n",
    "    lr_scheduler = ExponentialLR(optimizer, gamma=0.90)\n",
    "    trainer = create_supervised_trainer1(model.to(device), optimizer, criterion, device=device)\n",
    "    evaluator = create_supervised_evaluator1(model.to(device), metrics={'BCELoss': Loss(criterion)}, device=device)\n",
    "\n",
    "    if log_interval is None:\n",
    "        e = Events.ITERATION_COMPLETED\n",
    "        log_interval = 1\n",
    "    else:\n",
    "        e = Events.ITERATION_COMPLETED(every=log_interval)\n",
    "\n",
    "    desc = \"loss: {:.4f} | lr: {:.4f}\"\n",
    "    pbar = tqdm(\n",
    "        initial=0, leave=False, total=len(train_loader),\n",
    "        desc=desc.format(0, lr)\n",
    "    )\n",
    "\n",
    "    @trainer.on(e)\n",
    "    def log_training_loss(engine):\n",
    "        pbar.refresh()\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.desc = desc.format(engine.state.output[0], lr)\n",
    "        pbar.update(log_interval)\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def update_lr_scheduler(engine):\n",
    "        lr_scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_training_results(engine):\n",
    "        evaluator.run(train_loader)\n",
    "        metrics = evaluator.state.metrics\n",
    "        avg_loss = metrics['BCELoss']\n",
    "        tqdm.write(\n",
    "            \"Train Epoch: {} BCE loss: {:.2f}\".format(engine.state.epoch, avg_loss)\n",
    "        )\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_validation_results(engine):\n",
    "        pbar.refresh()\n",
    "        evaluator.run(valid_loader)\n",
    "        metrics = evaluator.state.metrics\n",
    "        avg_loss = metrics['BCELoss']\n",
    "        tqdm.write(\n",
    "            \"Valid Epoch: {} BCE loss: {:.2f}\".format(engine.state.epoch, avg_loss)\n",
    "        )\n",
    "        pbar.n = pbar.last_print_n = 0\n",
    "\n",
    "\n",
    "    try:\n",
    "        trainer.run(train_loader, max_epochs=epochs)\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "    return model\n",
    "\n",
    "\n",
    "#开始训练\n",
    "model = run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 预测"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "# 定义测试数据集的dataset\n",
    "class TestTextDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer,max_len):\n",
    "\n",
    "        self.bert_encode = tokenizer\n",
    "        self.texts = df.text.values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        tokens,mask,tokens_len = self.get_token_mask(self.texts[idx],self.max_len)\n",
    "        return [torch.tensor(tokens),torch.tensor(mask),torch.tensor(tokens_len)]\n",
    "\n",
    "    def get_token_mask(self,text,max_len):\n",
    "\n",
    "        tokens = []\n",
    "        mask = []\n",
    "        text = self.bert_encode.encode(text)\n",
    "        size = len(text)\n",
    "        pads = self.bert_encode.encode(['PAD']*(max(0,max_len-size)))\n",
    "        tokens[:max(max_len,size)] = text[:max(max_len,size)]\n",
    "        tokens = tokens + pads[1:-1]\n",
    "        mask = [1]*size+[0]*len(pads[1:-1])\n",
    "        tokens_len = len(tokens)\n",
    "\n",
    "        return tokens,mask,tokens_len"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/102 [00:12<?, ?it/s]\u001B[A\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- 17\n",
      "++++: last_hidden_state ++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'permute'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-99-48dc6929f3b1>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtotal\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m         \u001B[0minputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ma\u001B[0m \u001B[1;32min\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m         \u001B[0mpreds\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     10\u001B[0m         \u001B[0mpredictions\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpreds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\IT\\code\\Python\\environment\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-95-834557d8ffdc>\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m     24\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"++++:\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mencoded_layers\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m\"++++\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m         \u001B[1;31m# encoded_layers = torch.from_numpy(encoded_layers)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 26\u001B[1;33m         \u001B[0mencoded_layers\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mencoded_layers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpermute\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m#将tensor的维度换位\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     27\u001B[0m         \"\"\"\n\u001B[0;32m     28\u001B[0m         \u001B[1;31m# pack_padded_sequence:压缩填充张量  去除末尾填充的PAD，防止PAD进入模型，浪费资源\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'str' object has no attribute 'permute'"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model.eval()\n",
    "predictions = []\n",
    "test_dataset = TestTextDataset(test,tokenizer=tokenizer,max_len=120)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=32,shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for idx , (inputs) in tqdm(enumerate(test_loader),total=len(test_loader)):\n",
    "        inputs = [a.to(device) for a in inputs]\n",
    "        preds = model(inputs)\n",
    "        predictions.append(preds.cpu().detach().numpy())\n",
    "\n",
    "predictions = np.vstack(predictions)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 查看并输出预测结果"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_subs.target = np.round(np.vstack(predictions)).astype(int)\n",
    "print(sample_subs.head(20))\n",
    "sample_subs.to_csv('submission.csv', index = False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}