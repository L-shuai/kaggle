{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataloader\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "'''\n",
    "Load the BERT tokenizer.\n",
    "'''\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "\n",
    "'''\n",
    "Dataloader\n",
    "For Training: Returns (Tweet, Input_id, Attention_mask, label)\n",
    "For Testing: Returns (Tweet, Input_id, Attention_mask)\n",
    "'''\n",
    "class mydataset():\n",
    "\n",
    "    def __init__(self, classification_df, name = 'train'):\n",
    "\n",
    "        super(mydataset).__init__()\n",
    "        self.name = name\n",
    "        self.tweet = []\n",
    "        self.Y = []\n",
    "\n",
    "        for index,rows in classification_df.iterrows():\n",
    "\n",
    "            tweet = rows['keyword'] + rows['location'] + rows['text']\n",
    "            self.tweet.append(''.join(tweet))\n",
    "\n",
    "            if name == 'train' or self.name == 'valid':\n",
    "                label = rows['target']\n",
    "                self.Y.append(label)\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        Tokenize all of the captions and map the tokens to thier word IDs, and get respective attention masks.\n",
    "        '''\n",
    "        self.input_ids, self.attention_masks = tokenize(self.tweet)\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        '''\n",
    "        For Captions, Input ids and Attention mask\n",
    "        '''\n",
    "        tweet = self.tweet[index]\n",
    "        input_id = self.input_ids[index]\n",
    "        attention_masks = self.attention_masks[index]\n",
    "\n",
    "\n",
    "        '''\n",
    "        For Labels during training\n",
    "        '''\n",
    "        if self.name == 'train' or self.name == 'valid' :\n",
    "            label = float(self.Y[index])\n",
    "\n",
    "            return tweet, input_id, attention_masks, torch.as_tensor(label).long()\n",
    "\n",
    "\n",
    "        else:\n",
    "            return tweet, input_id, attention_masks\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "tokenize all of the sentences and map the tokens to their word IDs.\n",
    "'''\n",
    "\n",
    "def tokenize(sequences):\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every caption...\n",
    "    for seq in sequences:\n",
    "        \"\"\"\n",
    "        1. encode仅返回input_ids\n",
    "        2. encode_plus返回所有的编码信息，具体如下：\n",
    "        ’input_ids:是单词在词典中的编码\n",
    "        ‘token_type_ids’:区分两个句子的编码（上句全为0，下句全为1）\n",
    "        ‘attention_mask’:指定对哪些词进行self-Attention操作\n",
    "        \"\"\"\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            seq,                       # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 32,           # Pad & truncate all sentences.\n",
    "                            truncation=True,\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',      # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids.append(encoded_dict['input_ids'])  #encoded_dict是encode_plus返回的字典\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "\n",
    "    return input_ids, attention_masks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training samples:  6852\n",
      "Number of Validation samples:  761\n",
      "         id                keyword              location  \\\n",
      "2687   6113               hellfire                         \n",
      "7219   1780  buildings%20on%20fire            Auburn, AL   \n",
      "1465   6200               hijacker               Halifax   \n",
      "6225   9864            traumatised                Sweden   \n",
      "5532   7069               meltdown              KSU 2017   \n",
      "...     ...                    ...                   ...   \n",
      "3779  10520               wildfire  Lancaster California   \n",
      "6774   6173                 hijack               Kolkata   \n",
      "7491   4639   emergency%20services                 ?????   \n",
      "4488   8781                  siren       California, USA   \n",
      "864    3663                destroy         New York City   \n",
      "\n",
      "                                                   text  target  \n",
      "2687  Hellfire! We donÛªt even want to think about ...       0  \n",
      "7219  Ton of smoke coming out of one of the new apar...       1  \n",
      "1465  Remove the http://t.co/l4wJHz4AJ6 and Linkury ...       0  \n",
      "6225  @Ruddyyyyyy @JamieGriff97 Jamie is too traumat...       0  \n",
      "5532  @DmoneyDemi I had my meltdown yesterday.  I'm ...       0  \n",
      "...                                                 ...     ...  \n",
      "3779  The Latest: More homes razed by Northern Calif...       1  \n",
      "6774              Stay cautious. http://t.co/JeJC9XcTMp       0  \n",
      "7491  beyond stressed beyond hysteria into the grey ...       0  \n",
      "4488  Can you save\\nCan you save my\\nCan you save my...       0  \n",
      "864   Watch These Super Strong Magnets Destroy Every...       1  \n",
      "\n",
      "[6851 rows x 5 columns]\n",
      "         id          keyword           location  \\\n",
      "185   10207  violent%20storm     3rd Eye Chakra   \n",
      "6920   8181         rescuers            Phoenix   \n",
      "5683  10723            wreck      United States   \n",
      "6261   5068           famine            JamDung   \n",
      "4881   8393        sandstorm                USA   \n",
      "...     ...              ...                ...   \n",
      "1231   6118         hellfire                      \n",
      "4129   6342         hostages            Midwest   \n",
      "405    4270         drowning  South Korea GMT+9   \n",
      "17     3212          deluged  Karachi, Pakistan   \n",
      "27      270        ambulance            Karachi   \n",
      "\n",
      "                                                   text  target  \n",
      "185   Violent video: Ukraine rioters brutally beat p...       1  \n",
      "6920  When Rescuers Found Him He Was Barely Alive. B...       1  \n",
      "5683  AmazonDeals: Skylanders Trap Team: Flip Wreck ...       0  \n",
      "6261       Ppl like unu feast today and famine tomorrow       0  \n",
      "4881  Watch This Airport Get Swallowed Up By A Sands...       1  \n",
      "...                                                 ...     ...  \n",
      "1231  The Prophet (peace be upon him) said 'Save you...       0  \n",
      "4129  Sinjar Massacre Yazidis Blast Lack of Action O...       1  \n",
      "405   SometimesI can't even breathe well\\nI feel lik...       0  \n",
      "17    #Glimpses: Hyderabad deluged by heavy rainfall...       1  \n",
      "27    Twelve feared killed in Pakistani air ambulanc...       1  \n",
      "\n",
      "[762 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Shuffle and split 10 percent data for Validation set\n",
    "'''\n",
    "train_csv = pd.read_csv('dataset/train.csv', keep_default_na = False)\n",
    "# 打乱\n",
    "train_csv = train_csv.sample(frac=1).reset_index(drop=True)\n",
    "ninetyfive_percent  = round(0.90*(len(train_csv)))\n",
    "train_data = train_csv.iloc[:ninetyfive_percent]\n",
    "valid_data = train_csv.iloc[ninetyfive_percent:]\n",
    "\n",
    "print('Number of Training samples: ', len(train_data))\n",
    "print('Number of Validation samples: ',len(valid_data))\n",
    "# print(train_data)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train , x_valid = train_test_split(train_csv, test_size=0.1,random_state=2020)\n",
    "print(x_train)\n",
    "print(x_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\IT\\code\\Python\\environment\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train Dataloader\n",
    "'''\n",
    "train_dataset = mydataset(train_data, name = 'train')\n",
    "train_dataloader = data.DataLoader(train_dataset, shuffle= True, batch_size = 32, num_workers=0,pin_memory=True)\n",
    "\n",
    "\n",
    "'''\n",
    "Validation_Dataloader\n",
    "'''\n",
    "validation_dataset = mydataset(valid_data, name = 'valid')\n",
    "validation_dataloader = data.DataLoader(validation_dataset, shuffle= True, batch_size = 32, num_workers=0,pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\IT\\code\\Python\\environment\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test Dataloader\n",
    "'''\n",
    "test_csv = pd.read_csv('dataset/test.csv', keep_default_na = False)\n",
    "test_dataset = mydataset(test_csv , name = 'test')\n",
    "test_dataloader = data.DataLoader(test_dataset, shuffle= False, batch_size = 1, num_workers=0,pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Definition\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
    "'''\n",
    "# Transformers 已经实现好了用来分类的模型，我们这里就不自己编写了，直接使用 BertForSequenceClassification 调用预训练模型\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", #12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, #Number of Classes 二分类\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, data_loader, valid_loader, criterion, optimizer, lr_scheduler, modelpath, device, epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss= []\n",
    "    valid_loss = []\n",
    "    valid_acc = []\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss = 0.0\n",
    "\n",
    "\n",
    "        for batch_num, (tweet, input_id, attention_masks, target) in enumerate(tqdm(data_loader)):\n",
    "\n",
    "            input_ids, attention_masks, target = input_id.to(device), attention_masks.to(device), target.to(device)\n",
    "\n",
    "            '''\n",
    "            Compute output and loss from BERT\n",
    "            '''\n",
    "            loss, logits = model(input_ids,\n",
    "                             token_type_ids=None,\n",
    "                             attention_mask=attention_masks,\n",
    "                             labels=target,\n",
    "                             return_dict=False\n",
    "                                )\n",
    "\n",
    "            '''\n",
    "            Take Step\n",
    "            '''\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            '''\n",
    "            Clip the norm of the gradients to 1.0. This is to help prevent the \"exploding gradients\" problem.\n",
    "            '''\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            '''\n",
    "            linear_schedule_with_warmup take step after each batch\n",
    "            '''\n",
    "            lr_scheduler.step()\n",
    "\n",
    "\n",
    "        training_loss = avg_loss/len(data_loader)\n",
    "\n",
    "        print('Epoch: ', epoch+1)\n",
    "        print('training loss = ', training_loss)\n",
    "        train_loss.append(training_loss)\n",
    "\n",
    "        '''\n",
    "        Check performance on validation set after an Epoch\n",
    "        '''\n",
    "        validation_loss, top1_acc= test_classify(model, valid_loader, criterion, device)\n",
    "        valid_loss.append(validation_loss)\n",
    "        valid_acc.append(top1_acc)\n",
    "\n",
    "\n",
    "        '''\n",
    "        save model checkpoint after every epoch\n",
    "        '''\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            }, modelpath)\n",
    "\n",
    "    return train_loss, valid_loss, valid_acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Function to perform inference on validation set\n",
    "Returns: validation loss, top1 accuracy\n",
    "'''\n",
    "\n",
    "def test_classify(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    top1_accuracy = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_num, (tweet, input_id, attention_masks, target) in enumerate(valid_loader):\n",
    "\n",
    "        input_ids, attention_masks, target = input_id.to(device), attention_masks.to(device), target.to(device)\n",
    "\n",
    "        '''\n",
    "        Compute output and loss from BERT\n",
    "        '''\n",
    "        loss, logits = model(input_ids,\n",
    "                         token_type_ids=None,\n",
    "                         attention_mask=attention_masks,\n",
    "                         labels=target,\n",
    "                         return_dict=False)\n",
    "\n",
    "        test_loss.extend([loss.item()]*input_id.size()[0])\n",
    "\n",
    "        predictions = F.softmax(logits, dim=1)\n",
    "\n",
    "        _, top1_pred_labels = torch.max(predictions,1)\n",
    "        top1_pred_labels = top1_pred_labels.view(-1)\n",
    "\n",
    "        top1_accuracy += torch.sum(torch.eq(top1_pred_labels, target)).item()\n",
    "        total += len(target)\n",
    "\n",
    "    print('Validation Loss: {:.4f}\\tTop 1 Validation Accuracy: {:.4f}'.format(np.mean(test_loss), top1_accuracy/total))\n",
    "\n",
    "    return np.mean(test_loss), top1_accuracy/total\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 超参数\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "'''\n",
    "Loss Function\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "Optimizer\n",
    "'''\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "\n",
    "'''\n",
    "Number of training epochs. The BERT authors recommend between 2 and 4. Increasing the number of epochs with BERT will increase overfitting the training set, as it can be seen from the loss plot later.\n",
    "'''\n",
    "num_Epochs = 4\n",
    "\n",
    "'''\n",
    "Create the learning rate scheduler.\n",
    "Total number of training steps is [number of batches] x [number of epochs].\n",
    "'''\n",
    "total_steps = len(train_dataloader) * num_Epochs\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0,  num_training_steps = total_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 开始训练\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 162/215 [26:23<08:38,  9.78s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-18-b77291eaba01>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mmodelpath\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'saved_checkpoint_'\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mmodelname\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m \u001B[0mtrain_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalid_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalid_acc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_dataloader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalidation_dataloader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlr_scheduler\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodelpath\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnum_Epochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-16-43ecbe102aef>\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(model, data_loader, valid_loader, criterion, optimizer, lr_scheduler, modelpath, device, epochs)\u001B[0m\n\u001B[0;32m     32\u001B[0m             '''\n\u001B[0;32m     33\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 34\u001B[1;33m             \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     35\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\IT\\code\\Python\\environment\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    253\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    254\u001B[0m                 inputs=inputs)\n\u001B[1;32m--> 255\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    256\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    257\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\IT\\code\\Python\\environment\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    147\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[0;32m    148\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 149\u001B[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001B[0m\u001B[0;32m    150\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 导入日志模块 创建更漂亮的输出消息\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "modelname = 'BERT'\n",
    "modelpath = 'saved_checkpoint_'+modelname\n",
    "\n",
    "train_loss, valid_loss, valid_acc = train(model, train_dataloader, validation_dataloader, criterion, optimizer, lr_scheduler, modelpath, device, epochs = num_Epochs)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 可视化\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_loss(epochs, train_loss, test_loss, title):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    x = np.arange(1,epochs+1)\n",
    "    plt.plot(x, train_loss, label = 'Training Loss')\n",
    "    plt.plot(x, test_loss, label = 'Validation Loss')\n",
    "    plt.xlabel('Epochs', fontsize =16)\n",
    "    plt.ylabel('Loss', fontsize =16)\n",
    "    plt.title(title,fontsize =16)\n",
    "    plt.legend(fontsize=16)\n",
    "\n",
    "\n",
    "def plot_acc(epochs,test_acc):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    x = np.arange(1,epochs+1)\n",
    "    plt.plot(x, test_acc)\n",
    "    plt.xlabel('Epochs', fontsize =16)\n",
    "    plt.ylabel('Test Accuracy', fontsize =16)\n",
    "    plt.title('Test Accuracy v/s Epochs',fontsize =16)\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plot_loss(num_Epochs, train_loss, valid_loss, title='Loss plot')\n",
    "plot_acc(num_Epochs, valid_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 预测并输出\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(model, test_loader, device):\n",
    "    model.eval()\n",
    "    target = []\n",
    "    for batch_num, (captions, input_id, attention_masks) in enumerate(test_loader):\n",
    "\n",
    "\n",
    "        input_ids, attention_masks = input_id.to(device), attention_masks.to(device)\n",
    "\n",
    "        '''\n",
    "        Compute prediction outputs from BERT\n",
    "        '''\n",
    "        output_dictionary = model(input_ids,\n",
    "                         token_type_ids=None,\n",
    "                         attention_mask=attention_masks,\n",
    "                         return_dict=True)\n",
    "\n",
    "        predictions = F.softmax(output_dictionary['logits'], dim=1)\n",
    "\n",
    "        _, top1_pred_labels = torch.max(predictions,1)\n",
    "        top1_pred_labels = top1_pred_labels.item()\n",
    "        target.append(top1_pred_labels)\n",
    "\n",
    "\n",
    "    make_csv(target)\n",
    "\n",
    "\n",
    "def make_csv(target):\n",
    "    test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "    my_submission = pd.DataFrame({'id': test.id, 'target': target})\n",
    "    my_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "\n",
    "predict(model, test_dataloader, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}