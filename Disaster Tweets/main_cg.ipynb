{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 带有灾难推文的自然语言处理\n",
    ">预测哪些推文是关于真实灾难的，哪些不是\n",
    ">来源：https://www.kaggle.com/search?competitionId=17777\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/test.csv\n",
      "./dataset/sample_submission.csv\n",
      "./dataset/train.csv\n"
     ]
    }
   ],
   "source": [
    "# 导包\n",
    "import numpy as np\n",
    "import pandas as pd #data processing, CSV file I/O\n",
    "import os\n",
    "\n",
    "from transformers import BertTokenizer,BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm #查看进度\n",
    "from argparse import ArgumentParser\n",
    "# Ignite帮助您在几行代码中编写紧凑但功能齐全的train循环\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.engine.engine import Engine, State, Events\n",
    "from ignite.handlers import EarlyStopping\n",
    "from ignite.contrib.handlers import TensorboardLogger, ProgressBar\n",
    "from ignite.utils import convert_tensor\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for dirname,_,filenames in os.walk('./dataset'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname,filename))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 读取数据&加载Bert分词器"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e39fe204a82b46d4a5d0bf68d9d6272c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11283a2516524b6b9859a3bad550c4b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18e1e31bcb1d463a8534485180ae3209"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81181ed39ecf4744882aa70a7dac0cde"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def readfiles():\n",
    "    path = 'dataset'\n",
    "    train = pd.read_csv(os.path.join(path,'train.csv'))\n",
    "    test = pd.read_csv(os.path.join(path,'test.csv'))\n",
    "    sample_subs = pd.read_csv(os.path.join(path,'sample_submission.csv'))\n",
    "    return train,test,sample_subs\n",
    "\n",
    "train,test,sample_subs = readfiles()\n",
    "# print(sample_subs)\n",
    "\n",
    "\n",
    "#加载Bert分词器\n",
    "from transformers import BertTokenizer\n",
    "def Bert_Tokenizer(model_name):\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = Bert_Tokenizer('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 制作dataset数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#自定义dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer,max_len):\n",
    "        self.bert_encode = tokenizer\n",
    "        self.texts = df.text.values\n",
    "        self.labels = df.target.values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens,mask,tokens_len = self.get_token_mask(self.texts[idx],self.max_len)\n",
    "        return [torch.tensor(tokens),torch.tensor(mask),torch.tensor(tokens_len)]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def get_token_mask(self,text,max_len):\n",
    "        tokens = [] #分词\n",
    "        mask = [] #和网络中的ip掩码功能一样\n",
    "        text = self.bert_encode.encode(text)\n",
    "        size = len(text)\n",
    "        pads = self.bert_encode.encode(['PAD']*max(0,max_len-size)) #不足的补PAD\n",
    "        tokens[:max(max_len,size)] = text[:max(max_len,size)] #超出的部分截断\n",
    "        tokens = tokens + pads[1:-1]\n",
    "        mask = [1]*size + [0]*len(pads[1:-1])\n",
    "        tokens_len = len(tokens)\n",
    "        return tokens,mask,tokens_len\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#加载数据集\n",
    "def get_data_loaders():\n",
    "    x_train,x_valid = train_test_split(train,test_size=0.1,random_state=2022)\n",
    "    train_dataset = TextDataset(x_train,tokenizer=tokenizer,max_len=120)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=1,shuffle=True)\n",
    "    valid_dataset = TextDataset(x_valid,tokenizer=tokenizer,max_len=120)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=1,shuffle=True)\n",
    "\n",
    "    return train_loader,valid_loader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 搭建Bert模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class MixedBertModel(nn.Module):\n",
    "    def __init__(self,pre_trained='bert-base-uncased'):\n",
    "        # 待验证\n",
    "        super(MixedBertModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pre_trained)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        # 双向LSTM\n",
    "        \"\"\"\n",
    "        input_size – 输入数据的大小，也就是前面例子中每个单词向量的长度\n",
    "        hidden_size – 隐藏层的大小（即隐藏层节点数量），输出向量的维度等于隐藏节点数\n",
    "        num_layers – recurrent layer的数量，默认等于1。\n",
    "        \"\"\"\n",
    "        self.LSTM = nn.LSTM(input_size=self.hidden_size,hidden_size=self.hidden_size,bidirectional=True)\n",
    "        # 分类器\n",
    "        self.clf = nn.Linear(in_features=self.hidden_size*2,out_features=1)\n",
    "\n",
    "    def forward(self,input):\n",
    "        encode_layers,pooled_output = self.bert(input_ids = input[0],attention_mask = input[1])\n",
    "        encode_layers = encode_layers.permute(1,0,2) #将tensor的维度换位\n",
    "        \"\"\"\n",
    "        # pack_padded_sequence:压缩填充张量  去除末尾填充的PAD，防止PAD进入模型，浪费资源\n",
    "        LSTM的输出:enc_hiddens,(last_hidden,last_cell)\n",
    "        last_hidden为最后1个time step的隐状态结果\n",
    "        last_cell为最后1个time step的cell状态结果\n",
    "        \"\"\"\n",
    "        enc_hiddens,(last_hidden,last_cell) = self.LSTM(pack_padded_sequence(input=encode_layers,lengths=input[2]))\n",
    "        output_hidden = torch.cat((last_hidden[0],last_hidden[1]),dim=1)\n",
    "        # 避免过拟合，并增强模型的泛化能力\n",
    "        output_hidden = F.dropout(output_hidden,0.2)\n",
    "        output = self.clf(output_hidden)\n",
    "        return F.sigmoid(output)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 预处理"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#将数据集转为tensor类型\n",
    "def _prepare_batch(batch,device=None,non_blocking=False):\n",
    "    x,y = batch\n",
    "    return (convert_tensor(x,device=device,non_blocking=non_blocking),\n",
    "            convert_tensor(y,device=device,non_blocking=non_blocking))\n",
    "\n",
    "# 创建监督学习\n",
    "def create_supervised_trainer1(model,optimizer,loss_fn,metrics={},device=None):\n",
    "\n",
    "    def _update(engine,batch):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()#每一轮开始前  梯度都要清零\n",
    "        x,y = _prepare_batch(batch,device=device)\n",
    "        y_pred = model(x)#模型输出值\n",
    "        loss = loss_fn(y_pred,y.float()) #计算损失值\n",
    "        loss.backward()\n",
    "        optimizer.step()#更新参数值\n",
    "        return loss.item(),y_pred,y\n",
    "\n",
    "    def _metrics_transform(output):\n",
    "        return output[1],output[2]\n",
    "\n",
    "    engine = Engine(_update)\n",
    "\n",
    "    for name,metric in metrics.items():\n",
    "        metric._output_transform = _metrics_transform\n",
    "        metric.attach(engine,name)\n",
    "\n",
    "    return engine\n",
    "\n",
    "def create_supervised_evaluator1(model, metrics=None,\n",
    "                                device=None, non_blocking=False,\n",
    "                                prepare_batch=_prepare_batch,\n",
    "                                output_transform=lambda x, y, y_pred: (y_pred, y,)):\n",
    "    metrics = metrics or {}\n",
    "\n",
    "    if device:\n",
    "        model\n",
    "\n",
    "    def _inference(engine, batch):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x, y = prepare_batch(batch, device=device, non_blocking=non_blocking)\n",
    "            y_pred = model(x)\n",
    "            return output_transform(x, y.float(), y_pred)\n",
    "\n",
    "    engine = Engine(_inference)\n",
    "\n",
    "    for name, metric in metrics.items():\n",
    "        metric.attach(engine, name)\n",
    "\n",
    "    return engine"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "训练模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e91457b03134a55a810b6e380c25efe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "loss: 0.0000 | lr: 0.0000:   0%|          | 0/6851 [00:00<?, ?it/s]Current run is terminating due to exception: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>\n",
      "Engine run is terminating due to exception: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2277/3680251638.py\", line 59, in run\n",
      "    trainer.run(train_loader, max_epochs=epochs)\n",
      "  File \"/headless/anaconda3/envs/pytorch/lib/python3.8/site-packages/ignite/engine/engine.py\", line 698, in run\n",
      "    return self._internal_run()\n",
      "  File \"/headless/anaconda3/envs/pytorch/lib/python3.8/site-packages/ignite/engine/engine.py\", line 771, in _internal_run\n",
      "    self._handle_exception(e)\n",
      "  File \"/headless/anaconda3/envs/pytorch/lib/python3.8/site-packages/ignite/engine/engine.py\", line 466, in _handle_exception\n",
      "    raise e\n",
      "  File \"/headless/anaconda3/envs/pytorch/lib/python3.8/site-packages/ignite/engine/engine.py\", line 741, in _internal_run\n",
      "    time_taken = self._run_once_on_dataset()\n",
      "  File \"/headless/anaconda3/envs/pytorch/lib/python3.8/site-packages/ignite/engine/engine.py\", line 845, in _run_once_on_dataset\n",
      "    self._handle_exception(e)\n",
      "  File \"/headless/anaconda3/envs/pytorch/lib/python3.8/site-packages/ignite/engine/engine.py\", line 466, in _handle_exception\n",
      "    raise e\n",
      "  File \"/headless/anaconda3/envs/pytorch/lib/python3.8/site-packages/ignite/engine/engine.py\", line 798, in _run_once_on_dataset\n",
      "    self.state.batch = next(self._dataloader_iter)\n",
      "  File \"/headless/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 435, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/headless/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 475, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/headless/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/headless/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 85, in default_collate\n",
      "    raise TypeError(default_collate_err_msg_format.format(elem_type))\n",
      "TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run(log_interval=100,epochs=2,lr=0.000006):\n",
    "    train_loader,valid_loader = get_data_loaders()\n",
    "    model = MixedBertModel()\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    criterion = nn.BCELoss() #损失函数\n",
    "    optimizer = AdamW(model.parameters(),lr=lr) #优化器\n",
    "    # 学习率衰减，每个epoch中lr都乘以gamma\n",
    "    lr_scheduler = ExponentialLR(optimizer,gamma=0.90)\n",
    "    trainer = create_supervised_trainer1(model.to(device),optimizer,criterion,device=device)\n",
    "    evaluator = create_supervised_evaluator1(model.to(device),metrics={\"BCELoss\":Loss(criterion)},device=device)\n",
    "\n",
    "    if log_interval is None:\n",
    "        e = Events.ITERATION_COMPLETED\n",
    "        log_interval=1\n",
    "    else:\n",
    "        e = Events.ITERATION_COMPLETED(every=log_interval)\n",
    "\n",
    "    desc = \"loss: {:.4f} | lr: {:.4f}\"\n",
    "    pbar = tqdm(\n",
    "        initial=0, leave=False, total=len(train_loader),\n",
    "        desc=desc.format(0, lr)\n",
    "    )\n",
    "\n",
    "    @trainer.on(e)\n",
    "    def log_training_loss(engine):\n",
    "        pbar.refresh()\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.desc = desc.format(engine.state.output[0], lr)\n",
    "        pbar.update(log_interval)\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def update_lr_scheduler(engine):\n",
    "        lr_scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_training_results(engine):\n",
    "        evaluator.run(train_loader)\n",
    "        metrics = evaluator.state.metrics\n",
    "        avg_loss = metrics['BCELoss']\n",
    "        tqdm.write(\n",
    "            \"Train Epoch: {} BCE loss: {:.2f}\".format(engine.state.epoch, avg_loss)\n",
    "        )\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_validation_results(engine):\n",
    "        pbar.refresh()\n",
    "        evaluator.run(valid_loader)\n",
    "        metrics = evaluator.state.metrics\n",
    "        avg_loss = metrics['BCELoss']\n",
    "        tqdm.write(\n",
    "            \"Valid Epoch: {} BCE loss: {:.2f}\".format(engine.state.epoch, avg_loss)\n",
    "        )\n",
    "        pbar.n = pbar.last_print_n = 0\n",
    "\n",
    "\n",
    "    try:\n",
    "        trainer.run(train_loader, max_epochs=epochs)\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "    return model\n",
    "\n",
    "\n",
    "#开始训练\n",
    "model = run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 预测"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 定义测试数据集的dataset\n",
    "class TestTextDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer,max_len):\n",
    "\n",
    "        self.bert_encode = tokenizer\n",
    "        self.texts = df.text.values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        tokens,mask,tokens_len = self.get_token_mask(self.texts[idx],self.max_len)\n",
    "        return [torch.tensor(tokens),torch.tensor(mask),torch.tensor(tokens_len)]\n",
    "\n",
    "    def get_token_mask(self,text,max_len):\n",
    "\n",
    "        tokens = []\n",
    "        mask = []\n",
    "        text = self.bert_encode.encode(text)\n",
    "        size = len(text)\n",
    "        pads = self.bert_encode.encode(['PAD']*(max(0,max_len-size)))\n",
    "        tokens[:max(max_len,size)] = text[:max(max_len,size)]\n",
    "        tokens = tokens + pads[1:-1]\n",
    "        mask = [1]*size+[0]*len(pads[1:-1])\n",
    "        tokens_len = len(tokens)\n",
    "\n",
    "        return tokens,mask,tokens_len"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model.eval()\n",
    "predictions = []\n",
    "test_dataset = TestTextDataset(test,tokenizer=tokenizer,max_len=120)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=32,shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for idx , (inputs) in tqdm(enumerate(test_loader),total=len(test_loader)):\n",
    "        inputs = [a.to(device) for a in inputs]\n",
    "        preds = model(inputs)\n",
    "        predictions.append(preds.cpu().detach().numpy())\n",
    "\n",
    "predictions = np.vstack(predictions)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 查看并输出预测结果"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_subs.target = np.round(np.vstack(predictions)).astype(int)\n",
    "print(sample_subs.head(20))\n",
    "sample_subs.to_csv('submission.csv', index = False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}