{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataloader\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "'''\n",
    "Load the BERT tokenizer.\n",
    "'''\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "\n",
    "'''\n",
    "Dataloader\n",
    "For Training: Returns (Tweet, Input_id, Attention_mask, label)\n",
    "For Testing: Returns (Tweet, Input_id, Attention_mask)\n",
    "'''\n",
    "class mydataset():\n",
    "\n",
    "    def __init__(self, classification_df, name = 'train'):\n",
    "\n",
    "        super(mydataset).__init__()\n",
    "        self.name = name\n",
    "        self.tweet = []\n",
    "        self.Y = []\n",
    "\n",
    "        for index,rows in classification_df.iterrows():\n",
    "\n",
    "            tweet = rows['keyword'] + rows['location'] + rows['text']\n",
    "            self.tweet.append(''.join(tweet))\n",
    "\n",
    "            if name == 'train' or self.name == 'valid':\n",
    "                label = rows['target']\n",
    "                self.Y.append(label)\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        Tokenize all of the captions and map the tokens to thier word IDs, and get respective attention masks.\n",
    "        '''\n",
    "        self.input_ids, self.attention_masks = tokenize(self.tweet)\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        '''\n",
    "        For Captions, Input ids and Attention mask\n",
    "        '''\n",
    "        tweet = self.tweet[index]\n",
    "        input_id = self.input_ids[index]\n",
    "        attention_masks = self.attention_masks[index]\n",
    "\n",
    "\n",
    "        '''\n",
    "        For Labels during training\n",
    "        '''\n",
    "        if self.name == 'train' or self.name == 'valid' :\n",
    "            label = float(self.Y[index])\n",
    "\n",
    "            return tweet, input_id, attention_masks, torch.as_tensor(label).long()\n",
    "\n",
    "\n",
    "        else:\n",
    "            return tweet, input_id, attention_masks\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "tokenize all of the sentences and map the tokens to their word IDs.\n",
    "'''\n",
    "\n",
    "def tokenize(sequences):\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every caption...\n",
    "    for seq in sequences:\n",
    "\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            seq,                       # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 32,           # Pad & truncate all sentences.\n",
    "                            truncation=True,\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',      # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "\n",
    "    return input_ids, attention_masks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training samples:  6852\n",
      "Number of Validation samples:  761\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Shuffle and split 10 percent data for Validation set\n",
    "'''\n",
    "train_csv = pd.read_csv('dataset/train.csv', keep_default_na = False)\n",
    "train_csv = train_csv.sample(frac=1).reset_index(drop=True)\n",
    "ninetyfive_percent  = round(0.90*(len(train_csv)))\n",
    "train_data = train_csv.iloc[:ninetyfive_percent]\n",
    "valid_data = train_csv.iloc[ninetyfive_percent:]\n",
    "\n",
    "print('Number of Training samples: ', len(train_data))\n",
    "print('Number of Validation samples: ',len(valid_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\IT\\code\\Python\\environment\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train Dataloader\n",
    "'''\n",
    "train_dataset = mydataset(train_data, name = 'train')\n",
    "train_dataloader = data.DataLoader(train_dataset, shuffle= True, batch_size = 32, num_workers=0,pin_memory=True)\n",
    "\n",
    "\n",
    "'''\n",
    "Validation_Dataloader\n",
    "'''\n",
    "validation_dataset = mydataset(valid_data, name = 'valid')\n",
    "validation_dataloader = data.DataLoader(validation_dataset, shuffle= True, batch_size = 32, num_workers=0,pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "'''\n",
    "Test Dataloader\n",
    "'''\n",
    "test_csv = pd.read_csv('dataset/test.csv', keep_default_na = False)\n",
    "test_dataset = mydataset(test_csv , name = 'test')\n",
    "test_dataloader = data.DataLoader(test_dataset, shuffle= False, batch_size = 1, num_workers=0,pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Definition\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
    "'''\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", #12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, #Number of Classes\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练模型\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train(model, data_loader, valid_loader, criterion, optimizer, lr_scheduler, modelpath, device, epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss= []\n",
    "    valid_loss = []\n",
    "    valid_acc = []\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss = 0.0\n",
    "\n",
    "\n",
    "        for batch_num, (tweet, input_id, attention_masks, target) in enumerate(data_loader):\n",
    "\n",
    "            input_ids, attention_masks, target = input_id.to(device), attention_masks.to(device), target.to(device)\n",
    "\n",
    "            '''\n",
    "            Compute output and loss from BERT\n",
    "            '''\n",
    "            loss, logits = model(input_ids,\n",
    "                             token_type_ids=None,\n",
    "                             attention_mask=attention_masks,\n",
    "                             labels=target,\n",
    "                             return_dict=False\n",
    "                                )\n",
    "\n",
    "            '''\n",
    "            Take Step\n",
    "            '''\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            '''\n",
    "            Clip the norm of the gradients to 1.0. This is to help prevent the \"exploding gradients\" problem.\n",
    "            '''\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            '''\n",
    "            linear_schedule_with_warmup take step after each batch\n",
    "            '''\n",
    "            lr_scheduler.step()\n",
    "\n",
    "\n",
    "        training_loss = avg_loss/len(data_loader)\n",
    "\n",
    "        print('Epoch: ', epoch+1)\n",
    "        print('training loss = ', training_loss)\n",
    "        train_loss.append(training_loss)\n",
    "\n",
    "        '''\n",
    "        Check performance on validation set after an Epoch\n",
    "        '''\n",
    "        validation_loss, top1_acc= test_classify(model, valid_loader, criterion, device)\n",
    "        valid_loss.append(validation_loss)\n",
    "        valid_acc.append(top1_acc)\n",
    "\n",
    "\n",
    "        '''\n",
    "        save model checkpoint after every epoch\n",
    "        '''\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            }, modelpath)\n",
    "\n",
    "    return train_loss, valid_loss, valid_acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Function to perform inference on validation set\n",
    "Returns: validation loss, top1 accuracy\n",
    "'''\n",
    "\n",
    "def test_classify(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    top1_accuracy = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_num, (tweet, input_id, attention_masks, target) in enumerate(valid_loader):\n",
    "\n",
    "        input_ids, attention_masks, target = input_id.to(device), attention_masks.to(device), target.to(device)\n",
    "\n",
    "        '''\n",
    "        Compute output and loss from BERT\n",
    "        '''\n",
    "        loss, logits = model(input_ids,\n",
    "                         token_type_ids=None,\n",
    "                         attention_mask=attention_masks,\n",
    "                         labels=target,\n",
    "                         return_dict=False)\n",
    "\n",
    "        test_loss.extend([loss.item()]*input_id.size()[0])\n",
    "\n",
    "        predictions = F.softmax(logits, dim=1)\n",
    "\n",
    "        _, top1_pred_labels = torch.max(predictions,1)\n",
    "        top1_pred_labels = top1_pred_labels.view(-1)\n",
    "\n",
    "        top1_accuracy += torch.sum(torch.eq(top1_pred_labels, target)).item()\n",
    "        total += len(target)\n",
    "\n",
    "    print('Validation Loss: {:.4f}\\tTop 1 Validation Accuracy: {:.4f}'.format(np.mean(test_loss), top1_accuracy/total))\n",
    "\n",
    "    return np.mean(test_loss), top1_accuracy/total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 超参数\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "'''\n",
    "Loss Function\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "Optimizer\n",
    "'''\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "\n",
    "'''\n",
    "Number of training epochs. The BERT authors recommend between 2 and 4. Increasing the number of epochs with BERT will increase overfitting the training set, as it can be seen from the loss plot later.\n",
    "'''\n",
    "num_Epochs = 4\n",
    "\n",
    "'''\n",
    "Create the learning rate scheduler.\n",
    "Total number of training steps is [number of batches] x [number of epochs].\n",
    "'''\n",
    "total_steps = len(train_dataloader) * num_Epochs\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0,  num_training_steps = total_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 开始训练\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modelname = 'BERT'\n",
    "modelpath = 'saved_checkpoint_'+modelname\n",
    "\n",
    "train_loss, valid_loss, valid_acc = train(model, train_dataloader, validation_dataloader, criterion, optimizer, lr_scheduler, modelpath, device, epochs = num_Epochs)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 可视化\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_loss(epochs, train_loss, test_loss, title):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    x = np.arange(1,epochs+1)\n",
    "    plt.plot(x, train_loss, label = 'Training Loss')\n",
    "    plt.plot(x, test_loss, label = 'Validation Loss')\n",
    "    plt.xlabel('Epochs', fontsize =16)\n",
    "    plt.ylabel('Loss', fontsize =16)\n",
    "    plt.title(title,fontsize =16)\n",
    "    plt.legend(fontsize=16)\n",
    "\n",
    "\n",
    "def plot_acc(epochs,test_acc):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    x = np.arange(1,epochs+1)\n",
    "    plt.plot(x, test_acc)\n",
    "    plt.xlabel('Epochs', fontsize =16)\n",
    "    plt.ylabel('Test Accuracy', fontsize =16)\n",
    "    plt.title('Test Accuracy v/s Epochs',fontsize =16)\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plot_loss(num_Epochs, train_loss, valid_loss, title='Loss plot')\n",
    "plot_acc(num_Epochs, valid_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 预测并输出\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(model, test_loader, device):\n",
    "    model.eval()\n",
    "    target = []\n",
    "    for batch_num, (captions, input_id, attention_masks) in enumerate(test_loader):\n",
    "\n",
    "\n",
    "        input_ids, attention_masks = input_id.to(device), attention_masks.to(device)\n",
    "\n",
    "        '''\n",
    "        Compute prediction outputs from BERT\n",
    "        '''\n",
    "        output_dictionary = model(input_ids,\n",
    "                         token_type_ids=None,\n",
    "                         attention_mask=attention_masks,\n",
    "                         return_dict=True)\n",
    "\n",
    "        predictions = F.softmax(output_dictionary['logits'], dim=1)\n",
    "\n",
    "        _, top1_pred_labels = torch.max(predictions,1)\n",
    "        top1_pred_labels = top1_pred_labels.item()\n",
    "        target.append(top1_pred_labels)\n",
    "\n",
    "\n",
    "    make_csv(target)\n",
    "\n",
    "\n",
    "def make_csv(target):\n",
    "    test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "    my_submission = pd.DataFrame({'id': test.id, 'target': target})\n",
    "    my_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "\n",
    "predict(model, test_dataloader, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}